{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text\n",
    "from io import StringIO\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, display_html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split our X and y<br>\n",
    "do the capital X, lowercase y thing for train test and split<br>\n",
    "X is the data frame of the features, y is a series of the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_Xy (df, column):\n",
    "    '''\n",
    "    Take in a DataFrame (train, validate, test) and return X and y; .\n",
    "    df: train, validate or  test. Select one\n",
    "    column: which column you want to  stratify on. Ex. stratify on 'churn'\n",
    "    return X_df, y_df.\n",
    "    Example:\n",
    "    X_validate, y_validate = split_Xy(validate, 'survived') \n",
    "    '''\n",
    "    X_df = df.drop(columns= column)\n",
    "    y_df = df[[column]]\n",
    "    return X_df, y_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_performs (X_df, y_df, model):\n",
    "    '''\n",
    "    Take in a X_df, y_df and model  and fit the model , make a prediction, calculate score (accuracy), \n",
    "    confusion matrix, rates, clasification report.\n",
    "    X_df: train, validate or  test. Select one\n",
    "    y_df: it has to be the same as X_df.\n",
    "    model: name of your model that you prevously created \n",
    "    \n",
    "    Example:\n",
    "    mmodel_performs (X_train, y_train, model1)\n",
    "    '''\n",
    "\n",
    "    #prediction\n",
    "    pred = model.predict(X_df)\n",
    "\n",
    "    #score = accuracy\n",
    "    acc = model.score(X_df, y_df)\n",
    "\n",
    "    #conf Matrix\n",
    "    conf = confusion_matrix(y_df, pred)\n",
    "    mat =  pd.DataFrame ((confusion_matrix(y_df, pred )),index = ['actual_no_churn','actual_churn'], columns =['pred_no_churn','pred_churn' ])\n",
    "    rubric_df = pd.DataFrame([['True Negative', 'False positive'], ['False Negative', 'True Positive']], columns=mat.columns, index=mat.index)\n",
    "    cf = rubric_df + ': ' + mat.values.astype(str)\n",
    "\n",
    "    #assign the values\n",
    "    tp = conf[1,1]\n",
    "    fp =conf[0,1] \n",
    "    fn= conf[1,0]\n",
    "    tn =conf[0,0]\n",
    "\n",
    "    #calculate the rate\n",
    "    tpr = tp/(tp+fn)\n",
    "    fpr = fp/(fp+tn)\n",
    "    tnr = tn/(tn+fp)\n",
    "    fnr = fn/(fn+tp)\n",
    "\n",
    "    #classification report\n",
    "    clas_rep =pd.DataFrame(classification_report(y_df, pred, output_dict=True)).T\n",
    "    clas_rep.rename(index={'0': \"No Churn\", '1': \"Churn\"}, inplace = True)\n",
    "    print(f'''\n",
    "    The accuracy for our model is {acc:.4%}\n",
    "    The True Positive Rate is {tpr:.3%},    The False Positive Rate is {fpr:.3%},\n",
    "    The True Negative Rate is {tnr:.3%},    The False Negative Rate is {fnr:.3%}\n",
    "    ________________________________________________________________________________\n",
    "    ''')\n",
    "    print('''\n",
    "    The positive is  'Churn'\n",
    "    Confusion Matrix\n",
    "    ''')\n",
    "    display(cf)\n",
    "    print('''\n",
    "    ________________________________________________________________________________\n",
    "    \n",
    "    Classification Report:\n",
    "    ''')\n",
    "    display(clas_rep)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dec_tree(model, X_df):\n",
    "    '''\n",
    "    Plot a decision tree.\n",
    "    Take in a model, X_df  \n",
    "    model: name of your model that you prevously created \n",
    "    X_df: train, validate or  test. Select one\n",
    "    \n",
    "    Example:\n",
    "    model.dec_tree(model1, X_train)\n",
    "    '''\n",
    "    plt.figure(figsize=(24, 12))\n",
    "    plot_tree(\n",
    "    model,\n",
    "    feature_names=X_df.columns.tolist(),\n",
    "    class_names=['died', 'survived'],\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare (model1, model2, X_df,y_df):\n",
    "    '''\n",
    "    Take in two models to compare their performance metrics.\n",
    "    X_df: train, validate or  test. Select one\n",
    "    y_df: it has to be the same as X_df.\n",
    "    model1: name of your first model that you want to compare  \n",
    "    model2: name of your second model that you want to compare\n",
    "    Example: \n",
    "    compare(logit2, logit4, X_validate, y_validate)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #prediction\n",
    "    pred1 = model1.predict(X_df)\n",
    "    pred2 = model2.predict(X_df)\n",
    "\n",
    "    #score = accuracy\n",
    "    acc1 = model1.score(X_df, y_df)\n",
    "    acc2 = model2.score(X_df, y_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #conf Matrix\n",
    "    #model 1\n",
    "    conf1 = confusion_matrix(y_df, pred1)\n",
    "    mat1 =  pd.DataFrame ((confusion_matrix(y_df, pred1 )),index = ['actual_no_churn','actual_churn'], columns =['pred_no_churn','pred_churn' ])\n",
    "    rubric_df = pd.DataFrame([['True Negative', 'False positive'], ['False Negative', 'True Positive']], columns=mat1.columns, index=mat1.index)\n",
    "    cf1 = rubric_df + ': ' + mat1.values.astype(str)\n",
    "    \n",
    "    #model2\n",
    "    conf2 = confusion_matrix(y_df, pred2)\n",
    "    mat2 =  pd.DataFrame ((confusion_matrix(y_df, pred2 )),index = ['actual_no_churn','actual_churn'], columns =['pred_no_churn','pred_churn' ])\n",
    "    cf2 = rubric_df + ': ' + mat2.values.astype(str)\n",
    "    #model 1\n",
    "    #assign the values\n",
    "    tp = conf1[1,1]\n",
    "    fp =conf1[0,1] \n",
    "    fn= conf1[1,0]\n",
    "    tn =conf1[0,0]\n",
    "\n",
    "    #calculate the rate\n",
    "    tpr1 = tp/(tp+fn)\n",
    "    fpr1 = fp/(fp+tn)\n",
    "    tnr1 = tn/(tn+fp)\n",
    "    fnr1 = fn/(fn+tp)\n",
    "\n",
    "    #model 2\n",
    "    #assign the values\n",
    "    tp = conf2[1,1]\n",
    "    fp =conf2[0,1] \n",
    "    fn= conf2[1,0]\n",
    "    tn =conf2[0,0]\n",
    "\n",
    "    #calculate the rate\n",
    "    tpr2 = tp/(tp+fn)\n",
    "    fpr2 = fp/(fp+tn)\n",
    "    tnr2 = tn/(tn+fp)\n",
    "    fnr2 = fn/(fn+tp)\n",
    "\n",
    "    #classification report\n",
    "    #model1\n",
    "    clas_rep1 =pd.DataFrame(classification_report(y_df, pred1, output_dict=True)).T\n",
    "    clas_rep1.rename(index={'0': \"no_churn\", '1': \"churn\"}, inplace = True)\n",
    "\n",
    "    #model2\n",
    "    clas_rep2 =pd.DataFrame(classification_report(y_df, pred2, output_dict=True)).T\n",
    "    clas_rep2.rename(index={'0': \"no_churn\", '1': \"churn\"}, inplace = True)\n",
    "    print(f'''\n",
    "    ******       Model 1  ******                                ******     Model 2  ****** \n",
    "    The accuracy for our model 1 is {acc1:.4%}            |   The accuracy for our model 2 is {acc2:.4%}  \n",
    "                                                        |\n",
    "    The True Positive Rate is {tpr1:.3%}                   |   The True Positive Rate is {tpr2:.3%}  \n",
    "    The False Positive Rate is {fpr1:.3%}                  |   The False Positive Rate is {fpr2:.3%} \n",
    "    The True Negative Rate is {tnr1:.3%}                   |   The True Negative Rate is {tnr2:.3%} \n",
    "    The False Negative Rate is {fnr1:.3%}                  |   The False Negative Rate is {fnr2:.3%}\n",
    "    _____________________________________________________________________________________________________________\n",
    "    ''')\n",
    "    print('''\n",
    "    The positive is  'churn'\n",
    "    Confusion Matrix\n",
    "    ''')\n",
    "    cf1_styler = cf1.style.set_table_attributes(\"style='display:inline'\").set_caption('Model 1')\n",
    "    cf2_styler = cf2.style.set_table_attributes(\"style='display:inline'\").set_caption('Model2')\n",
    "    space = \"\\xa0\" * 50\n",
    "    display_html(cf1_styler._repr_html_()+ space  + cf2_styler._repr_html_(), raw=True)\n",
    "    # print(display(cf1),\"           \", display(cf2))\n",
    "    \n",
    "    print('''\n",
    "    ________________________________________________________________________________\n",
    "    \n",
    "    Classification Report:\n",
    "    ''')\n",
    "     \n",
    "    clas_rep1_styler = clas_rep1.style.set_table_attributes(\"style='display:inline'\").set_caption('Model 1 Classification Report')\n",
    "    clas_rep2_styler = clas_rep2.style.set_table_attributes(\"style='display:inline'\").set_caption('Model 2 Classification Report')\n",
    "    space = \"\\xa0\" * 45\n",
    "    display_html(clas_rep1_styler._repr_html_()+ space  + clas_rep2_styler._repr_html_(), raw=True)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_metrics (model, name_dataset1, X, y, name_dataset2,  X2, y2 ):\n",
    "    '''\n",
    "    Take in a  model and compare the  performance metrics of  Train, Evaluate and Test (only 2).\n",
    "    model: the model that you want to compare\n",
    "    name_dataset1 : type :train, validate or  test. Select one, STRING\n",
    "    X: df test, validate or test\n",
    "    y: df test, validate or test\n",
    "    name_dataset2: type :train, validate or  test. Select one, STRING\n",
    "    X2: df2 test, validate or test\n",
    "    y2: df2 test, validate or test\n",
    "    \n",
    "    Example:\n",
    "    compare_metrics(logit2,'Train',X_train, y_train,'Test', X_test, y_test)\n",
    "    '''\n",
    "    \n",
    "    if name_dataset1.lower() != \"train\" and name_dataset1.lower() != \"validate\" and name_dataset1.lower() != \"test\" :\n",
    "        return print(\"incorrect name\")\n",
    "    if name_dataset2.lower() != \"train\" and name_dataset2.lower() != \"validate\" and name_dataset2.lower() != \"test\" :\n",
    "        return print(\"incorrect name\")\n",
    "    #prediction\n",
    "    pred_1 = model.predict(X)\n",
    "    pred_2 = model.predict(X2)\n",
    "\n",
    "    #score = accuracy\n",
    "    acc_1 = model.score(X, y)\n",
    "    acc_2 = model.score(X2, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #conf Matrix\n",
    "    #model 1\n",
    "    conf_1 = confusion_matrix(y, pred_1)\n",
    "    mat_1 =  pd.DataFrame ((confusion_matrix(y, pred_1 )),index = ['actual_no_churn','actual_churn'], columns =['pred_no_churn','pred_churn' ])\n",
    "    rubric_df = pd.DataFrame([['TN', 'FP'], ['FN', 'TP']], columns=mat_1.columns, index=mat_1.index)\n",
    "    cf_1 = rubric_df + ' : ' + mat_1.values.astype(str)\n",
    "    \n",
    "    #model2\n",
    "    conf_2 = confusion_matrix(y2, pred_2)\n",
    "    mat_2 =  pd.DataFrame ((confusion_matrix(y2, pred_2 )),index = ['actual_no_churn','actual_churn'], columns =['pred_no_churn','pred_churn' ])\n",
    "    cf_2 = rubric_df + ' : ' + mat_2.values.astype(str)\n",
    "    #model 1\n",
    "    #assign the values\n",
    "    tp = conf_1[1,1]\n",
    "    fp = conf_1[0,1] \n",
    "    fn = conf_1[1,0]\n",
    "    tn = conf_1[0,0]\n",
    "\n",
    "    #calculate the rate\n",
    "    tpr_1 = tp/(tp+fn)\n",
    "    fpr_1 = fp/(fp+tn)\n",
    "    tnr_1 = tn/(tn+fp)\n",
    "    fnr_1 = fn/(fn+tp)\n",
    "\n",
    "    #model 2\n",
    "    #assign the values\n",
    "    tp = conf_2[1,1]\n",
    "    fp = conf_2[0,1] \n",
    "    fn = conf_2[1,0]\n",
    "    tn = conf_2[0,0]\n",
    "\n",
    "    #calculate the rate\n",
    "    tpr_2 = tp/(tp+fn)\n",
    "    fpr_2 = fp/(fp+tn)\n",
    "    tnr_2 = tn/(tn+fp)\n",
    "    fnr_2 = fn/(fn+tp)\n",
    "\n",
    "    #classification report\n",
    "    #model1\n",
    "    clas_rep_1 =pd.DataFrame(classification_report(y, pred_1, output_dict=True)).T\n",
    "    clas_rep_1.rename(index={'0': \"no_churn\", '1': \"churn\"}, inplace = True)\n",
    "\n",
    "    #model2\n",
    "    clas_rep_2 =pd.DataFrame(classification_report(y2, pred_2, output_dict=True)).T\n",
    "    clas_rep_2.rename(index={'0': \"no_churn\", '1': \"churn\"}, inplace = True)\n",
    "    print(f'''\n",
    "    ******    {name_dataset1}       ******                              ******     {name_dataset2}    ****** \n",
    "       Overall Accuracy:  {acc_1:.2%}              |                Overall Accuracy:  {acc_2:.2%}  \n",
    "                                                \n",
    "    True Positive Rate:  {tpr_1:.2%}               |          The True Positive Rate:  {tpr_2:.2%}  \n",
    "    False Positive Rate:  {fpr_1:.2%}              |          The False Positive Rate:  {fpr_2:.2%} \n",
    "    True Negative Rate:  {tnr_1:.2%}               |          The True Negative Rate:  {tnr_2:.2%} \n",
    "    False Negative Rate:  {fnr_1:.2%}              |          The False Negative Rate:  {fnr_2:.2%}\n",
    "    _________________________________________________________________________________\n",
    "    ''')\n",
    "    print('''\n",
    "    Positive =  'churn'\n",
    "    Confusion Matrix\n",
    "    ''')\n",
    "    cf_1_styler = cf_1.style.set_table_attributes(\"style='display:inline'\").set_caption(f'{name_dataset1} Confusion Matrix')\n",
    "    cf_2_styler = cf_2.style.set_table_attributes(\"style='display:inline'\").set_caption(f'{name_dataset2} Confusion Matrix')\n",
    "    space = \"\\xa0\" * 50\n",
    "    display_html(cf_1_styler._repr_html_()+ space  + cf_2_styler._repr_html_(), raw=True)\n",
    "    print('''\n",
    "    ________________________________________________________________________________\n",
    "    \n",
    "    Classification Report:\n",
    "    ''')\n",
    "    clas_rep_1_styler = clas_rep_1.style.set_table_attributes(\"style='display:inline'\").set_caption(f'{name_dataset1} Classification Report')\n",
    "    clas_rep_2_styler = clas_rep_2.style.set_table_attributes(\"style='display:inline'\").set_caption(f'{name_dataset2} Classification Report')\n",
    "    space = \"\\xa0\" * 45\n",
    "    display_html(clas_rep_1_styler._repr_html_()+ space  + clas_rep_2_styler._repr_html_(), raw=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
